---
title: "Rsquared"
description: |
  RSquared can be a very misleading predictor
output: 
  distill::distill_article:
    toc: false
    toc_depth: 3
---

# Possible Issues from  R-Squared

R-Squared does a poor job of measure fit. Even when a model is completely correct, it can still be a relativity low fit. 
Alternatively when a model is incorrect, R-Squared could be still be very close to 1.



# Time to blow RSquared up[^4] ðŸ’¥

[^4]: <https://data.library.virginia.edu/is-r-squared-useless/>


[R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination) is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.

In R, we typically get R-squared by calling the summary function on a model object. Here's a quick example using simulated data:

```{r,echo=TRUE}
# independent variable
x <- 1:20 
# for reproducibility
set.seed(1) 
# dependent variable; function of x with random error
y <- 2 + 0.5*x + rnorm(20,0,3) 
# simple linear regression
mod <- lm(y~x)
# request just the r-squared value
summary(mod)$r.squared          
```

One way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:

$$
R^{2} =  \frac{\sum (\hat{y} â€“ \bar{\hat{y}})^{2}}{\sum (y â€“ \bar{y})^{2}}
$$

We can calculate it directly using our model object like so:

```{r, echo=TRUE}
# extract fitted (or predicted) values from model
f <- mod$fitted.values
# sum of squared fitted-value deviations
mss <- sum((f - mean(f))^2)
# sum of squared original-value deviations
tss <- sum((y - mean(y))^2)
# r-squared
mss/tss                      
```

# *R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct.* By making$Ïƒ^2$ large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.

What is $Ïƒ^2$? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between "almost" and "exact" is assumed to be a draw from a Normal distribution with mean 0 and some variance we call $Ïƒ^2$.

This statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is `sigma`. We then "apply" this function to a series of increasing $Ïƒ$ values and plot the results.

```{r, echo=TRUE}
r2.0 <- function(sig){
  # our predictor
  x <- seq(1,10,length.out = 100)   
  # our response; a function of x plus some random noise
  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) 
  # print the R-squared value
  summary(lm(y ~ x))$r.squared          
}
sigmas <- seq(0.5,20,length.out = 20)
 # apply our function to a series of sigma values
rout <- sapply(sigmas, r2.0)            
plot(rout ~ sigmas, type="b")
```

R-squared tanks hard with increasing sigma, even though the model is *completely correct* in every respect.


# *R-squared can be arbitrarily close to 1 when the model is totally wrong.*

The point being made is that R-squared does not measure goodness of fit.

```{r, echo=TRUE}
set.seed(1)
# our predictor is data from an exponential distribution
x <- rexp(50,rate=0.005)
# non-linear data generation
y <- (x-1)^2 * runif(50, min=0.8, max=1.2) 
# clearly non-linear
plot(x,y)				     
```

```{r,echo=TRUE}
summary(lm(y ~ x))$r.squared
```

It's very high at about 0.85, but the model is completely wrong. Using R-squared to justify the "goodness" of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.




# Possible Solutions 

Mean Square Error (MSE) is much more effective when used to measure prediciton error