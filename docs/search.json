{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-07T21:31:05-06:00"
    },
    {
      "path": "blog.html",
      "title": "Blog",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-07T21:31:05-06:00"
    },
    {
      "path": "FP.html",
      "title": "Random Forest",
      "description": "Predicting Bankrupcty using Random Forest\"\n",
      "author": [],
      "contents": "\r\n1 Company Bankruptcy Prediction Dataset\r\n\r\nWhat is Random Forest?\r\nRandom Forest is a algorithm that comes to a conclusion using many different decisions trees. The most common outcome for each indiviual decision tree helps to create the final output. While this algorithm may seem complex the core of this algorithm decision trees, have been used my most wether knowingly or not.\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(caret)\r\ntrainIndex <- createDataPartition(bankdata$Bankrupt., p = .6, list = FALSE, times = 1)\r\n\r\n\r\n\r\nGrab The Data\r\n\r\n\r\n#grab the data\r\nBankTrain <- bankdata[ trainIndex,]\r\nBankTest  <- bankdata[-trainIndex,]\r\n\r\n\r\n\r\nPerform PreProcessing on Data\r\n\r\n\r\npreProcValues <- preProcess(BankTrain, method = c(\"center\", \"scale\"))\r\n\r\ntrainTransformed <- predict(preProcValues, BankTrain)\r\n\r\npreProcValues <- preProcess(BankTest, method = c(\"center\", \"scale\"))\r\n\r\ntestTransformed <- predict(preProcValues, BankTest)\r\npsych::describe(testTransformed)\r\n\r\n\r\n\r\nFit Random Forest\r\n\r\n\r\nset.seed(1)\r\n\r\nBankRF<- train(\r\n  form = factor(Bankrupt.) ~ .,\r\n  data = BankTrain,\r\n  #here we add classProbs because we want probs\r\n  trControl = trainControl(method = \"cv\", number = 10,\r\n                           classProbs =  TRUE),\r\n  method = \"rf\",\r\n  tuneLength = 3)#why 3?\r\n\r\nBankRF\r\n\r\n\r\nRandom Forest \r\n\r\n4092 samples\r\n  95 predictor\r\n   2 classes: 'No', 'Yes' \r\n\r\nNo pre-processing\r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 3683, 3683, 3682, 3683, 3683, 3682, ... \r\nResampling results across tuning parameters:\r\n\r\n  mtry  Accuracy   Kappa    \r\n   2    0.9704306  0.1777315\r\n  48    0.9714097  0.3229255\r\n  95    0.9718975  0.3442062\r\n\r\nAccuracy was used to select the optimal model using the\r\n largest value.\r\nThe final value used for the model was mtry = 95.\r\n\r\nUse the test set to make a prediction\r\n\r\n\r\nBank_pred<-predict(BankRF,testTransformed)\r\nhead(BankRF)\r\n\r\n\r\n$method\r\n[1] \"rf\"\r\n\r\n$modelInfo\r\n$modelInfo$label\r\n[1] \"Random Forest\"\r\n\r\n$modelInfo$library\r\n[1] \"randomForest\"\r\n\r\n$modelInfo$loop\r\nNULL\r\n\r\n$modelInfo$type\r\n[1] \"Classification\" \"Regression\"    \r\n\r\n$modelInfo$parameters\r\n  parameter   class                         label\r\n1      mtry numeric #Randomly Selected Predictors\r\n\r\n$modelInfo$grid\r\nfunction(x, y, len = NULL, search = \"grid\") {\r\n                    if(search == \"grid\") {\r\n                      out <- data.frame(mtry = caret::var_seq(p = ncol(x),\r\n                                                              classification = is.factor(y),\r\n                                                              len = len))\r\n                    } else {\r\n                      out <- data.frame(mtry = unique(sample(1:ncol(x), size = len, replace = TRUE)))\r\n                    }\r\n                  }\r\n\r\n$modelInfo$fit\r\nfunction(x, y, wts, param, lev, last, classProbs, ...)\r\n                    randomForest::randomForest(x, y, mtry = min(param$mtry, ncol(x)), ...)\r\n<bytecode: 0x0000000046e77978>\r\n\r\n$modelInfo$predict\r\nfunction(modelFit, newdata, submodels = NULL)\r\n                    if(!is.null(newdata)) predict(modelFit, newdata) else predict(modelFit)\r\n<bytecode: 0x00000000480dc320>\r\n\r\n$modelInfo$prob\r\nfunction(modelFit, newdata, submodels = NULL)\r\n                    if(!is.null(newdata)) predict(modelFit, newdata, type = \"prob\") else predict(modelFit, type = \"prob\")\r\n<bytecode: 0x00000000482e0268>\r\n\r\n$modelInfo$predictors\r\nfunction(x, ...) {\r\n                    ## After doing some testing, it looks like randomForest\r\n                    ## will only try to split on plain main effects (instead\r\n                    ## of interactions or terms like I(x^2).\r\n                    varIndex <- as.numeric(names(table(x$forest$bestvar)))\r\n                    varIndex <- varIndex[varIndex > 0]\r\n                    varsUsed <- names(x$forest$ncat)[varIndex]\r\n                    varsUsed\r\n                  }\r\n\r\n$modelInfo$varImp\r\nfunction(object, ...){\r\n                    varImp <- randomForest::importance(object, ...)\r\n                    if(object$type == \"regression\") {\r\n                      if(\"%IncMSE\" %in% colnames(varImp)) {\r\n                        varImp <- as.data.frame(varImp[,\"%IncMSE\", drop = FALSE])\r\n                        colnames(varImp) <- \"Overall\"\r\n                      } else {\r\n                        varImp <- as.data.frame(varImp[,1, drop = FALSE])\r\n                        colnames(varImp) <- \"Overall\"\r\n                      }\r\n                    }\r\n                    else {\r\n                      retainNames <- levels(object$y)\r\n                      if(all(retainNames %in% colnames(varImp))) {\r\n                        varImp <- varImp[, retainNames, drop = FALSE]\r\n                      } else {\r\n                        varImp <- as.data.frame(varImp[,1, drop = FALSE])\r\n                        colnames(varImp) <- \"Overall\"\r\n                      }\r\n                    }\r\n\r\n                    out <- as.data.frame(varImp, stringsAsFactors = TRUE)\r\n                    if(dim(out)[2] == 2) {\r\n                      tmp <- apply(out, 1, mean)\r\n                      out[,1] <- out[,2] <- tmp\r\n                    }\r\n                    out\r\n                  }\r\n\r\n$modelInfo$levels\r\nfunction(x) x$classes\r\n\r\n$modelInfo$tags\r\n[1] \"Random Forest\"              \"Ensemble Model\"            \r\n[3] \"Bagging\"                    \"Implicit Feature Selection\"\r\n\r\n$modelInfo$sort\r\nfunction(x) x[order(x[,1]),]\r\n\r\n$modelInfo$oob\r\nfunction(x) {\r\n                    out <- switch(x$type,\r\n                                  regression =   c(sqrt(max(x$mse[length(x$mse)], 0)), x$rsq[length(x$rsq)]),\r\n                                  classification =  c(1 - x$err.rate[x$ntree, \"OOB\"],\r\n                                                      e1071::classAgreement(x$confusion[,-dim(x$confusion)[2]])[[\"kappa\"]]))\r\n                    names(out) <- if(x$type == \"regression\") c(\"RMSE\", \"Rsquared\") else c(\"Accuracy\", \"Kappa\")\r\n                    out\r\n                  }\r\n\r\n\r\n$modelType\r\n[1] \"Classification\"\r\n\r\n$results\r\n  mtry  Accuracy     Kappa  AccuracySD   KappaSD\r\n1    2 0.9704306 0.1777315 0.003345483 0.1219371\r\n2   48 0.9714097 0.3229255 0.005396402 0.1364479\r\n3   95 0.9718975 0.3442062 0.004778483 0.1136868\r\n\r\n$pred\r\nNULL\r\n\r\n$bestTune\r\n  mtry\r\n3   95\r\n\r\n\r\nHow can this be used?\r\nRandom Forest does a great job predicting wether a comapny will go bankrupt or not in this scenario by using provided measurements. This prediction can help many different groups in regards to the longevity of future investments. Being able to make a accurate prediction of wether a organization is likely to file for bankruptcy could assist investors and help make more informed investments.\r\n\r\n\r\nhttps://www.kaggle.com/fedesoriano/company-bankruptcy-prediction↩︎\r\n",
      "last_modified": "2021-12-07T21:31:04-06:00"
    },
    {
      "path": "index.html",
      "title": "Jesse A. Flowers",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-07T21:31:05-06:00"
    },
    {
      "path": "jesse.html",
      "title": "Jesse Flowers",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          mfeo\r\n          \r\n          \r\n          Home\r\n          Resume\r\n          Jesse\r\n          \r\n          \r\n          Projects\r\n           \r\n          ▾\r\n          \r\n          \r\n          RSquared\r\n          Machine Learning\r\n          Final Project\r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            Jesse Flowers\r\n          \r\n          \r\n            \r\n              I am a student at Mississippi State University, where I am currently working towards my Masters of Taxation with a minor in Data Analytics.\r\n            \r\n            \r\n              I am a student at Mississippi State University, where I am currently working towards my Masters of Taxation with a minor in Data Analytics.\r\n            \r\n          \r\n\r\n          \r\n            \r\n              \r\n                  \r\n                    \r\n                      LinkedIn\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                \r\n                              \r\n          \r\n\r\n          \r\n            \r\n              \r\n                                \r\n                  \r\n                    LinkedIn\r\n                  \r\n                \r\n                                \r\n                  \r\n                    GitHub\r\n                  \r\n                \r\n                              \r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2021-12-07T21:31:05-06:00"
    },
    {
      "path": "ML.html",
      "title": "K Nearest Neighbors (KNN)",
      "author": [],
      "contents": "\r\n\r\nWhat is KNN?\r\nK-Nearest Neighbors or (KNN) is a machine learning algorithm. KNN works by trying to predict uisng test data by calculating distance between the test data and training points observed. This can be adjusted using KNN’s tuning parameter (k) in order to increase or decrease proximity.\r\n\r\n(KNN) Using Iris Data Set\r\nSplit The Data 60/40\r\n\r\n\r\nlibrary(caret)\r\ntrainIndexKNN <- createDataPartition(iris$Species, p = .6, list = FALSE, times = 1)\r\n\r\n\r\n\r\nGrab The Data\r\n\r\n\r\n#grab the data\r\nirisTrainKNN <- iris[ trainIndexKNN,]\r\nirisTestKNN  <- iris[-trainIndexKNN,]\r\n\r\n\r\n\r\nPerform PreProcessing on Data\r\n\r\n\r\npreProcValuesKNN <- preProcess(irisTrainKNN, method = c(\"center\", \"scale\"))\r\n\r\ntrainTransformedKNN <- predict(preProcValuesKNN, irisTrainKNN)\r\n\r\npreProcValuesKNN <- preProcess(irisTestKNN, method = c(\"center\", \"scale\"))\r\n\r\ntestTransformedKNN <- predict(preProcValuesKNN, irisTestKNN)\r\npsych::describe(testTransformedKNN)\r\n\r\n\r\n             vars  n mean   sd median trimmed  mad   min  max range\r\nSepal.Length    1 60    0 1.00   0.03   -0.03 1.31 -1.73 2.02  3.75\r\nSepal.Width     2 60    0 1.00  -0.18    0.00 0.67 -2.43 2.98  5.41\r\nPetal.Length    3 60    0 1.00   0.36    0.00 0.98 -1.49 1.60  3.09\r\nPetal.Width     4 60    0 1.00   0.18   -0.02 1.44 -1.44 1.67  3.11\r\nSpecies*        5 60    2 0.82   2.00    2.00 1.48  1.00 3.00  2.00\r\n              skew kurtosis   se\r\nSepal.Length  0.17    -0.94 0.13\r\nSepal.Width   0.09     0.65 0.13\r\nPetal.Length -0.27    -1.48 0.13\r\nPetal.Width  -0.08    -1.40 0.13\r\nSpecies*      0.00    -1.55 0.11\r\n\r\nFit KNN\r\n\r\n\r\nknn_fit<-train(Species~.,\r\n               data=trainTransformedKNN,\r\n               method=\"knn\",\r\n               tuneGrid=data.frame(k=5))\r\n\r\nknn_fit\r\n\r\n\r\nk-Nearest Neighbors \r\n\r\n90 samples\r\n 4 predictor\r\n 3 classes: 'setosa', 'versicolor', 'virginica' \r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \r\nResampling results:\r\n\r\n  Accuracy   Kappa    \r\n  0.9281673  0.8910733\r\n\r\nTuning parameter 'k' was held constant at a value of 5\r\n\r\nUse the test set to make a prediction\r\n\r\n\r\nknn_pred<-predict(knn_fit,testTransformedKNN)\r\nhead(knn_pred)\r\n\r\n\r\n[1] setosa setosa setosa setosa setosa setosa\r\nLevels: setosa versicolor virginica\r\n\r\nRun ConfusionMatrix\r\n\r\n\r\nconfusionMatrix(knn_pred,testTransformedKNN$Species)\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n            Reference\r\nPrediction   setosa versicolor virginica\r\n  setosa         20          0         0\r\n  versicolor      0         20         1\r\n  virginica       0          0        19\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9833          \r\n                 95% CI : (0.9106, 0.9996)\r\n    No Information Rate : 0.3333          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.975           \r\n                                          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: setosa Class: versicolor Class: virginica\r\nSensitivity                 1.0000            1.0000           0.9500\r\nSpecificity                 1.0000            0.9750           1.0000\r\nPos Pred Value              1.0000            0.9524           1.0000\r\nNeg Pred Value              1.0000            1.0000           0.9756\r\nPrevalence                  0.3333            0.3333           0.3333\r\nDetection Rate              0.3333            0.3333           0.3167\r\nDetection Prevalence        0.3333            0.3500           0.3167\r\nBalanced Accuracy           1.0000            0.9875           0.9750\r\n\r\n\r\nHow can Machine Learning benefit Accountants?\r\nMachine learning in general can provide many benefits for accountants. WHile machine learning algorithims can not replace all human interation, its much more effective at completing repetative tasks. This allows for a much more accurate and timely accounting review.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-06T13:09:08-06:00"
    },
    {
      "path": "Resume.html",
      "title": "Resume",
      "author": [],
      "contents": "\r\n\r\nJesse A. Flowers\r\nAdkerson School of Accountancy\r\nMississippi State University\r\nJesseflow99@gmail.com\r\n\r\nEMPLOYMENT\r\nMiles Tax Service,\r\nStarkville, MS — Accountant February 2020- May 2021\r\nWalmart,\r\nStarkville, MS — Automotive Associate May 2018- March 2020\r\nCar Wash USA,\r\nOlive Branch, MS — Supervisor/Key Holder April 2015 -August 2018\r\nEDUCATION\r\nMississippi State University, Starkville, MS,\r\nMaster of Taxation (Minor in Data Analytics)\r\nGraduation Date July 2022\r\nMississippi State University, Starkville, MS,\r\nBachelors of Accountancy\r\nGraduation Date April 2021\r\nSkills and Abilities\r\n• Advised clients on business and personal tax decisions.\r\n• Resolved tax issues for corporate and personal tax clients.\r\n• Obtained extensive experience in multiple computer programs such as QuickBooks, Proseries, and Excel.\r\n• Executed large collaborative projects.\r\n• Written and oral communication skills.\r\n• Excellent collaborative skills.\r\nHonors and Organizations\r\n• Accounting and Finance Student Society of Mississippi State University\r\n• President’s list at Mississippi State\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-05T19:03:17-06:00"
    },
    {
      "path": "RSquared.html",
      "title": "Rsquared",
      "description": "RSquared can be a very misleading predictor\n",
      "author": [],
      "contents": "\r\nPossible Issues from R-Squared\r\nR-Squared does a poor job of measure fit. Even when a model is completely correct, it can still be a relativity low fit. Alternatively when a model is incorrect, R-Squared could be still be very close to 1.\r\nTime to blow RSquared up1 💥\r\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\r\nIn R, we typically get R-squared by calling the summary function on a model object. Here’s a quick example using simulated data:\r\n\r\n\r\n# independent variable\r\nx <- 1:20 \r\n# for reproducibility\r\nset.seed(1) \r\n# dependent variable; function of x with random error\r\ny <- 2 + 0.5*x + rnorm(20,0,3) \r\n# simple linear regression\r\nmod <- lm(y~x)\r\n# request just the r-squared value\r\nsummary(mod)$r.squared          \r\n\r\n\r\n[1] 0.6026682\r\n\r\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\r\n\\[\r\nR^{2} =  \\frac{\\sum (\\hat{y} – \\bar{\\hat{y}})^{2}}{\\sum (y – \\bar{y})^{2}}\r\n\\]\r\nWe can calculate it directly using our model object like so:\r\n\r\n\r\n# extract fitted (or predicted) values from model\r\nf <- mod$fitted.values\r\n# sum of squared fitted-value deviations\r\nmss <- sum((f - mean(f))^2)\r\n# sum of squared original-value deviations\r\ntss <- sum((y - mean(y))^2)\r\n# r-squared\r\nmss/tss                      \r\n\r\n\r\n[1] 0.6026682\r\n\r\nR-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(σ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\r\nWhat is \\(σ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between “almost” and “exact” is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(σ^2\\).\r\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then “apply” this function to a series of increasing \\(σ\\) values and plot the results.\r\n\r\n\r\nr2.0 <- function(sig){\r\n  # our predictor\r\n  x <- seq(1,10,length.out = 100)   \r\n  # our response; a function of x plus some random noise\r\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \r\n  # print the R-squared value\r\n  summary(lm(y ~ x))$r.squared          \r\n}\r\nsigmas <- seq(0.5,20,length.out = 20)\r\n # apply our function to a series of sigma values\r\nrout <- sapply(sigmas, r2.0)            \r\nplot(rout ~ sigmas, type=\"b\")\r\n\r\n\r\n\r\n\r\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\r\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\r\nThe point being made is that R-squared does not measure goodness of fit.\r\n\r\n\r\nset.seed(1)\r\n# our predictor is data from an exponential distribution\r\nx <- rexp(50,rate=0.005)\r\n# non-linear data generation\r\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \r\n# clearly non-linear\r\nplot(x,y)             \r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.8485146\r\n\r\nIt’s very high at about 0.85, but the model is completely wrong. Using R-squared to justify the “goodness” of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.\r\nPossible Solutions\r\nMean Square Error (MSE) is much more effective when used to measure prediciton error\r\n\r\nhttps://data.library.virginia.edu/is-r-squared-useless/↩︎\r\n",
      "last_modified": "2021-12-05T19:03:19-06:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
